{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahad555/Image_Colorization_CNN/blob/main/LLM_BeforeEvasion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLYH67y3c-aL"
      },
      "source": [
        "## **Cloning From Repository**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This step involves cloning the necessary repository that contains the datasets and scripts for the project.\n",
        "\n",
        "Fetch the repository that contains the Enron1, Enron2, and SMS datasets along with the relevant code for spam classification."
      ],
      "metadata": {
        "id": "fWkajGLBJ4Qw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAPGOvk6a8FW",
        "outputId": "00b6c21b-451d-469e-f8bb-817aac4e1f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IDaSec-project'...\n",
            "remote: Enumerating objects: 433, done.\u001b[K\n",
            "remote: Counting objects: 100% (267/267), done.\u001b[K\n",
            "remote: Compressing objects: 100% (211/211), done.\u001b[K\n",
            "remote: Total 433 (delta 137), reused 162 (delta 48), pack-reused 166 (from 1)\u001b[K\n",
            "Receiving objects: 100% (433/433), 41.69 MiB | 9.84 MiB/s, done.\n",
            "Resolving deltas: 100% (197/197), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --branch main https://github.com/paulinaeb/IDaSec-project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Required Libraries**\n",
        "Various Python libraries and modules are imported for data processing, model building, training, and evaluation.\n",
        "\n",
        "**Libraries:**\n",
        "\n",
        "**torch:** For building and training neural networks.\n",
        "\n",
        "**pandas:** Used for data manipulation.\n",
        "\n",
        "**matplotlib:** For plotting graphs.\n",
        "\n",
        "**transformers:** Hugging Face library to use DistilBERT for sequence classification.\n",
        "\n",
        "**sklearn:** For model evaluation and data preprocessing.\n",
        "\n",
        "**google.colab:** For mounting Google Drive if necessary for saving/loading models.\n",
        "\n",
        "**Device Selection:** The device is set to CUDA if a GPU is available, otherwise falls back to CPU."
      ],
      "metadata": {
        "id": "qhJcbTLBKVWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OT-8Cr_7VVR",
        "outputId": "ddfda21d-863a-48fb-c068-9370d4997270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import DistilBertTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import DistilBertModel\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "\n",
        "# Determine device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrdCQLLicamK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Overview and Inspection**\n",
        "\n",
        "\n",
        "*   This block prints out summary statistics for each dataset, including the shape, columns, and a few sample rows to ensure the data is loaded correctly.\n",
        "*   the paths to each dataset (train, validation, and test) are set and the datasets are loaded into pandas DataFrames for further processing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "17R1vJP_K1AM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqkAyb0LOEww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8949489-4f7e-43f1-970f-6667d6bb24fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ENRON1_TRAIN ===\n",
            "Shape: (3196, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  Subject: prom dress shopping hi , just wanted ...    ham\n",
            "1  Subject: hi agaain hello , welcome to pharm la...   spam\n",
            "2  Subject: feedback monitor error - meter 984132...    ham\n",
            "\n",
            "=== ENRON1_VAL ===\n",
            "Shape: (799, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  Subject: union carbide - seadrift hpl meter # ...    ham\n",
            "1  Subject: best choice rx - free online prescrip...   spam\n",
            "2  Subject: new nat gas delivery location pursuan...    ham\n",
            "\n",
            "=== ENRON1_TEST ===\n",
            "Shape: (999, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  Subject: unify / sitara enhancements i am comp...    ham\n",
            "1  Subject: weekend activity dated : june 2 thru ...    ham\n",
            "2  Subject: re : tittletattle secrets dn ' t ie n...   spam\n",
            "\n",
            "=== ENRON2_TRAIN ===\n",
            "Shape: (3727, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  Subject: membership in the nsf vince : karen m...    ham\n",
            "1  Subject: holiday gift thank you so much for yo...    ham\n",
            "2  Subject: re : real options vince , if you take...    ham\n",
            "\n",
            "=== ENRON2_VAL ===\n",
            "Shape: (932, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  Subject: re : video conference with ross mcint...    ham\n",
            "1  Subject: \" help millions \" - pledge today ! th...    ham\n",
            "2  Subject: re : lng may 19 decision john , this ...    ham\n",
            "\n",
            "=== ENRON2_TEST ===\n",
            "Shape: (1165, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  Subject: sevil yamin vince , do you want me to...    ham\n",
            "1  Subject: you need only 15 minutes to prepare f...   spam\n",
            "2  Subject: i need your help dear sir , in view o...   spam\n",
            "\n",
            "=== SMS_TRAIN ===\n",
            "Shape: (3565, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  What to think no one saying clearly. Ok leave ...    ham\n",
            "1  FREE RING TONE just text \\POLYS\\\" to 87131. Th...   spam\n",
            "2          Trust me. Even if isn't there, its there.    ham\n",
            "\n",
            "=== SMS_VAL ===\n",
            "Shape: (892, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  have got * few things to do. may be in * pub l...    ham\n",
            "1  Okie but i scared u say i fat... Then u dun wa...    ham\n",
            "2                          Wot is u up 2 then bitch?    ham\n",
            "\n",
            "=== SMS_TEST ===\n",
            "Shape: (1115, 2)\n",
            "Columns: ['email', 'target']\n",
            "Sample rows:\n",
            "                                               email target\n",
            "0  Oh right, ok. I'll make sure that i do loads o...    ham\n",
            "1                      I am in tirupur. call you da.    ham\n",
            "2             No that just means you have a fat head    ham\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Dataset file paths\n",
        "dataset_paths = {\n",
        "    \"enron1_train\": './IDaSec-project/dataset/enron1/enron1_train.csv',\n",
        "    \"enron1_val\": './IDaSec-project/dataset/enron1/enron1_val.csv',\n",
        "    \"enron1_test\": './IDaSec-project/dataset/enron1/enron1_test.csv',\n",
        "    \"enron2_train\": './IDaSec-project/dataset/enron2/enron2_train.csv',\n",
        "    \"enron2_val\": './IDaSec-project/dataset/enron2/enron2_val.csv',\n",
        "    \"enron2_test\": './IDaSec-project/dataset/enron2/enron2_test.csv',\n",
        "    \"sms_train\": './IDaSec-project/dataset/sms/train.csv',\n",
        "    \"sms_val\": './IDaSec-project/dataset/sms/val.csv',\n",
        "    \"sms_test\": './IDaSec-project/dataset/sms/test.csv',\n",
        "}\n",
        "\n",
        "# Load datasets into a dictionary\n",
        "datasets = {name: pd.read_csv(path) for name, path in dataset_paths.items()}\n",
        "\n",
        "# Display summary info\n",
        "for name, df in datasets.items():\n",
        "    print(f\"\\n=== {name.upper()} ===\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(\"Columns:\", list(df.columns))\n",
        "    print(\"Sample rows:\")\n",
        "    print(df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function to Load and Clean Data**\n",
        "\n",
        "The **load_and_clean** function to load CSV files, clean the data by encoding the labels, and return a DataFrame with only the email and target columns.\n",
        "\n",
        "**Shape:** Displays the number of rows and columns.\n",
        "\n",
        "**Label Distribution:** Shows the count of ham and spam labels.\n",
        "\n",
        "**Sample Rows:** Displays the first three rows to inspect the data."
      ],
      "metadata": {
        "id": "p3AWvhKCL1OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and convert labels\n",
        "def load_and_clean(path):\n",
        "    df = pd.read_csv(path)\n",
        "    df['target'] = df['target'].str.strip().str.lower().map({'ham': 0, 'spam': 1})\n",
        "    return df[['email', 'target']]\n",
        "\n",
        "# Load and process all datasets\n",
        "datasets = {name: load_and_clean(path) for name, path in dataset_paths.items()}\n",
        "\n",
        "# Preview cleaned data\n",
        "for name, df in datasets.items():\n",
        "    print(f\"\\n=== {name.upper()} ===\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(df['target'].value_counts())\n",
        "    print(df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_IvG_DPZ6kY",
        "outputId": "b6b19e50-6d1e-4aaa-89b8-c15fe8634a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ENRON1_TRAIN ===\n",
            "Shape: (3196, 2)\n",
            "target\n",
            "0    2260\n",
            "1     936\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  Subject: prom dress shopping hi , just wanted ...       0\n",
            "1  Subject: hi agaain hello , welcome to pharm la...       1\n",
            "2  Subject: feedback monitor error - meter 984132...       0\n",
            "\n",
            "=== ENRON1_VAL ===\n",
            "Shape: (799, 2)\n",
            "target\n",
            "0    565\n",
            "1    234\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  Subject: union carbide - seadrift hpl meter # ...       0\n",
            "1  Subject: best choice rx - free online prescrip...       1\n",
            "2  Subject: new nat gas delivery location pursuan...       0\n",
            "\n",
            "=== ENRON1_TEST ===\n",
            "Shape: (999, 2)\n",
            "target\n",
            "0    706\n",
            "1    293\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  Subject: unify / sitara enhancements i am comp...       0\n",
            "1  Subject: weekend activity dated : june 2 thru ...       0\n",
            "2  Subject: re : tittletattle secrets dn ' t ie n...       1\n",
            "\n",
            "=== ENRON2_TRAIN ===\n",
            "Shape: (3727, 2)\n",
            "target\n",
            "0    2769\n",
            "1     958\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  Subject: membership in the nsf vince : karen m...       0\n",
            "1  Subject: holiday gift thank you so much for yo...       0\n",
            "2  Subject: re : real options vince , if you take...       0\n",
            "\n",
            "=== ENRON2_VAL ===\n",
            "Shape: (932, 2)\n",
            "target\n",
            "0    693\n",
            "1    239\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  Subject: re : video conference with ross mcint...       0\n",
            "1  Subject: \" help millions \" - pledge today ! th...       0\n",
            "2  Subject: re : lng may 19 decision john , this ...       0\n",
            "\n",
            "=== ENRON2_TEST ===\n",
            "Shape: (1165, 2)\n",
            "target\n",
            "0    866\n",
            "1    299\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  Subject: sevil yamin vince , do you want me to...       0\n",
            "1  Subject: you need only 15 minutes to prepare f...       1\n",
            "2  Subject: i need your help dear sir , in view o...       1\n",
            "\n",
            "=== SMS_TRAIN ===\n",
            "Shape: (3565, 2)\n",
            "target\n",
            "0    3087\n",
            "1     478\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  What to think no one saying clearly. Ok leave ...       0\n",
            "1  FREE RING TONE just text \\POLYS\\\" to 87131. Th...       1\n",
            "2          Trust me. Even if isn't there, its there.       0\n",
            "\n",
            "=== SMS_VAL ===\n",
            "Shape: (892, 2)\n",
            "target\n",
            "0    772\n",
            "1    120\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  have got * few things to do. may be in * pub l...       0\n",
            "1  Okie but i scared u say i fat... Then u dun wa...       0\n",
            "2                          Wot is u up 2 then bitch?       0\n",
            "\n",
            "=== SMS_TEST ===\n",
            "Shape: (1115, 2)\n",
            "target\n",
            "0    966\n",
            "1    149\n",
            "Name: count, dtype: int64\n",
            "                                               email  target\n",
            "0  Oh right, ok. I'll make sure that i do loads o...       0\n",
            "1                      I am in tirupur. call you da.       0\n",
            "2             No that just means you have a fat head       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Data Preprocessing and Merging Datasets**\n",
        "\n",
        "A **preprocessing function** for the text data, extracting metadata, encoding labels, and normalizing the metadata. It also combines the preprocessed datasets into a single **training**, **validation**, and **test dataset.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Preprocessing Function (preprocess_df):**\n",
        "\n",
        "**Text Cleaning:** The email text column is cleaned by converting all text to lowercase and removing any non-word characters using a regular expression.\n",
        "\n",
        "**Label Encoding:** The target labels (spam and ham) are mapped to binary values (1 for spam, 0 for ham).\n",
        "\n",
        "**Metadata Extraction:**\n",
        "\n",
        "\n",
        "*   Extracts the length of the subject line from the email.\n",
        "\n",
        "*   Calculates the total length of the email text.\n",
        "\n",
        "*   **Return:** The function returns the processed DataFrame and the scaler used for normalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3zK1Vz54MXvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocessing function\n",
        "def preprocess_df(df, text_col='email', label_col='target'):\n",
        "    df = df.copy()\n",
        "    # Clean text\n",
        "    df[text_col] = df[text_col].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "    # Encode labels\n",
        "    df[label_col] = df[label_col].map({'spam': 1, 'ham': 0})\n",
        "\n",
        "    # Extract metadata\n",
        "    df['subject_length'] = df[text_col].apply(lambda x: len(re.match(r'subject:.*?(?=\\n)', x, re.DOTALL).group(0)) if re.match(r'subject:.*?(?=\\n)', x, re.DOTALL) else 0)\n",
        "    df['text_length'] = df[text_col].str.len()\n",
        "\n",
        "    # Normalize metadata\n",
        "    scaler = StandardScaler()\n",
        "    metadata_cols = ['subject_length', 'text_length']\n",
        "    df[metadata_cols] = scaler.fit_transform(df[metadata_cols])\n",
        "\n",
        "    return df, scaler\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocessed_datasets = {}\n",
        "scalers = {}\n",
        "for name, df in datasets.items():\n",
        "    preprocessed_datasets[name], scalers[name] = preprocess_df(df)\n",
        "\n",
        "# Combine datasets\n",
        "train_df = pd.concat([\n",
        "    preprocessed_datasets['enron1_train'],\n",
        "    preprocessed_datasets['enron2_train'],\n",
        "    preprocessed_datasets['sms_train']\n",
        "]).reset_index(drop=True)\n",
        "val_df = pd.concat([\n",
        "    preprocessed_datasets['enron1_val'],\n",
        "    preprocessed_datasets['enron2_val'],\n",
        "    preprocessed_datasets['sms_val']\n",
        "]).reset_index(drop=True)\n",
        "test_df = pd.concat([\n",
        "    preprocessed_datasets['enron1_test'],\n",
        "    preprocessed_datasets['enron2_test'],\n",
        "    preprocessed_datasets['sms_test']\n",
        "]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "5uapE5mC8_wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization of Text Data**\n",
        "**Tokenizing** the email text data using the **DistilBERT** tokenizer, preparing it for input to the **DistilBERT** model.\n",
        "\n",
        "Converting the email text data into tokenized format using the DistilBERT tokenizer, ensuring the data is properly **formatted** for model input (padding and truncation as necessary).\n",
        "\n",
        "**DistilBERT Tokenizer:** The pre-trained DistilBERT tokenizer (distilbert-base-uncased) is loaded from the Hugging Face model hub.\n",
        "\n",
        "This **Tokenizer** is specifically designed for the DistilBERT model and handles tasks like tokenization, padding, and truncation.\n",
        "\n"
      ],
      "metadata": {
        "id": "sn3AhtDQNldg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize_data(texts, max_length=128):\n",
        "    return tokenizer(\n",
        "        texts.tolist(),\n",
        "        max_length=max_length,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "# Tokenize combined datasets\n",
        "train_encodings = tokenize_data(train_df['email'])\n",
        "val_encodings = tokenize_data(val_df['email'])\n",
        "test_encodings = tokenize_data(test_df['email'])"
      ],
      "metadata": {
        "id": "WPmlkyaF9CnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Dataset Class and DataLoader Preparation**\n",
        "A custom Dataset class to handle **tokenized** data and **metadata**, and prepares the **DataLoader** objects for efficient batching during model training and evaluation.\n",
        "\n",
        "**SpamDataset Class:**\n",
        "\n",
        "A custom subclass of torch.utils.data.Dataset that is designed to handle the tokenized data (encodings), additional metadata (metadata), and target labels (labels).\n",
        "\n",
        "**__init__ method:** Initializes the dataset with tokenized data, metadata, and labels.\n",
        "\n",
        "**__len__ method:** Returns the length of the dataset (the number of samples).\n",
        "\n",
        "**__getitem__ method:** Retrieves the item at the specified index (idx). This includes:\n",
        "\n"
      ],
      "metadata": {
        "id": "STC6bhkZN-Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, encodings, metadata, labels):\n",
        "        self.encodings = encodings\n",
        "        self.metadata = metadata\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['metadata'] = torch.tensor(self.metadata[idx], dtype=torch.float)\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# Prepare metadata\n",
        "metadata_cols = ['subject_length', 'text_length']\n",
        "train_metadata = train_df[metadata_cols].values\n",
        "val_metadata = val_df[metadata_cols].values\n",
        "test_metadata = test_df[metadata_cols].values\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SpamDataset(train_encodings, train_metadata, train_df['target'].values)\n",
        "val_dataset = SpamDataset(val_encodings, val_metadata, val_df['target'].values)\n",
        "test_dataset = SpamDataset(test_encodings, test_metadata, test_df['target'].values)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "sXS_sX279Uyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Model Class**\n",
        "\n",
        "\n",
        "\n",
        "**Dataset Description:**\n",
        "\n",
        " A custom model class that uses DistilBERT for text processing and incorporates metadata for enhanced prediction. The model combines the outputs from both text and metadata using weighted contributions.\n",
        "  \n",
        "  \n",
        "\n",
        "**Model Class (DistilBERTWithMetadata):**\n",
        "\n",
        "  **DistilBertModel:** Loads the pre-trained **DistilBERT** model for text representation.\n",
        "\n",
        "  A fully connected layer that projects **metadata** (with metadata_dim features) to 64 dimensions.\n",
        "\n",
        "  **Classifier:** A fully connected layer that takes the combined output of **BERT** and **metadata** features (768 dimensions from BERT + 64 from metadata) and outputs logits for 2 classes (spam and ham).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C01qPI4HOJwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model class with increased dropout\n",
        "class DistilBERTWithMetadata(nn.Module):\n",
        "    def __init__(self, metadata_dim, dropout=0.3):  # Increased dropout from 0.1 to 0.3\n",
        "        super(DistilBERTWithMetadata, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.metadata_fc = nn.Linear(metadata_dim, 64)  # Project metadata to 64 dims\n",
        "        self.classifier = nn.Linear(768 + 64, 2)  # 768 (BERT) + 64 (metadata)\n",
        "        self.text_weight = 0.8\n",
        "        self.metadata_weight = 0.2\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, metadata):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.last_hidden_state[:, 0]  # [CLS] token\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        metadata_output = torch.relu(self.metadata_fc(metadata))\n",
        "        metadata_output = self.dropout(metadata_output)\n",
        "\n",
        "        weighted_text = self.text_weight * pooled_output\n",
        "        weighted_metadata = self.metadata_weight * metadata_output\n",
        "        combined = torch.cat((weighted_text, weighted_metadata), dim=-1)\n",
        "\n",
        "        logits = self.classifier(combined)\n",
        "        return logits\n",
        "\n",
        "model = DistilBERTWithMetadata(metadata_dim=len(metadata_cols))"
      ],
      "metadata": {
        "id": "W5l_BZ6x9X9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Initialization, Training, and Evaluation Setup**\n",
        "\n",
        "Initialize the model with the required configurations, set up the optimizer and scheduler, and define the functions for training and evaluating the model. The training loop includes early stopping and model checkpoint saving based on validation loss.\n",
        "\n",
        "### **Model Class (DistilBERTWithMetadata):**\n",
        "\n",
        "A custom neural network that combines **DistilBERT** with additional metadata (subject length, text length) for classification.\n",
        "\n",
        "\n",
        "\n",
        "Model, Optimizer, and Loss Function Initialization:\n",
        "\n",
        "**Model:** The **DistilBERTWithMetadata** class is initialized with the number of metadata features.\n",
        "\n",
        "**Optimizer:** Uses AdamW with weight decay for regularization.\n",
        "\n",
        "**Loss Function:** Cross-entropy loss is used for binary classification.\n",
        "\n",
        "\n",
        "Training and Evaluation Functions:\n",
        "\n",
        "**train_epoch:** Defines the training loop for a single epoch, including loss computation, backpropagation, and optimization.\n",
        "\n",
        "\n",
        "**Model Saving:** Saves the model if validation and Training loss improves.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x0rb7ic_OcQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model class with increased dropout\n",
        "class DistilBERTWithMetadata(nn.Module):\n",
        "    def __init__(self, metadata_dim, dropout=0.3):  # Increased dropout from 0.1 to 0.3\n",
        "        super(DistilBERTWithMetadata, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.metadata_fc = nn.Linear(metadata_dim, 64)  # Project metadata to 64 dims\n",
        "        self.classifier = nn.Linear(768 + 64, 2)  # 768 (BERT) + 64 (metadata)\n",
        "        self.text_weight = 0.8\n",
        "        self.metadata_weight = 0.2\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, metadata):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.last_hidden_state[:, 0]  # [CLS] token\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        metadata_output = torch.relu(self.metadata_fc(metadata))\n",
        "        metadata_output = self.dropout(metadata_output)\n",
        "\n",
        "        weighted_text = self.text_weight * pooled_output\n",
        "        weighted_metadata = self.metadata_weight * metadata_output\n",
        "        combined = torch.cat((weighted_text, weighted_metadata), dim=-1)\n",
        "\n",
        "        logits = self.classifier(combined)\n",
        "        return logits\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)  # Force remount to avoid \"already mounted\" issue\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DistilBERTWithMetadata(metadata_dim=len(metadata_cols)).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Added weight decay\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "# Learning rate scheduler\n",
        "num_epochs = 10\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        metadata = batch['metadata'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, metadata)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return total_loss / len(data_loader), correct / total\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            metadata = batch['metadata'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, metadata)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return total_loss / len(data_loader), correct / total\n",
        "\n",
        "# Training loop with model and loss saving, including early stopping\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "best_val_loss = float('inf')\n",
        "patience = 2\n",
        "epochs_no_improve = 0\n",
        "save_path = '/content/drive/My Drive/distilbert_finetuned.pt'\n",
        "train_losses_path = '/content/drive/My Drive/train_losses.txt'\n",
        "val_losses_path = '/content/drive/My Drive/val_losses.txt'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, scheduler)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, device)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    # Print only the four average metrics\n",
        "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f'Model saved to {save_path}')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    # Early stopping check\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "        break\n",
        "\n",
        "# Save average training and validation losses to separate text files\n",
        "with open(train_losses_path, 'w') as f:\n",
        "    for loss in train_losses:\n",
        "        f.write(f'{loss:.4f}\\n')\n",
        "print(f'Training losses saved to {train_losses_path}')\n",
        "\n",
        "with open(val_losses_path, 'w') as f:\n",
        "    for loss in val_losses:\n",
        "        f.write(f'{loss:.4f}\\n')\n",
        "print(f'Validation losses saved to {val_losses_path}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN-QFtFF9r5a",
        "outputId": "b7bd8959-b988-4329-eb5d-69eb161c840e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Epoch 1: Train Loss: 0.1061, Train Acc: 0.9591, Val Loss: 0.0704, Val Acc: 0.9821\n",
            "Model saved to /content/drive/My Drive/distilbert_finetuned.pt\n",
            "Epoch 2: Train Loss: 0.0232, Train Acc: 0.9932, Val Loss: 0.0707, Val Acc: 0.9825\n",
            "Epoch 3: Train Loss: 0.0079, Train Acc: 0.9980, Val Loss: 0.0679, Val Acc: 0.9813\n",
            "Model saved to /content/drive/My Drive/distilbert_finetuned.pt\n",
            "Epoch 4: Train Loss: 0.0050, Train Acc: 0.9985, Val Loss: 0.0620, Val Acc: 0.9863\n",
            "Model saved to /content/drive/My Drive/distilbert_finetuned.pt\n",
            "Epoch 5: Train Loss: 0.0025, Train Acc: 0.9993, Val Loss: 0.1528, Val Acc: 0.9722\n",
            "Epoch 6: Train Loss: 0.0028, Train Acc: 0.9995, Val Loss: 0.0783, Val Acc: 0.9874\n",
            "Early stopping triggered after 6 epochs\n",
            "Training losses saved to /content/drive/My Drive/train_losses.txt\n",
            "Validation losses saved to /content/drive/My Drive/val_losses.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation and Classification Report**\n",
        "\n",
        "**Purpose:** Evaluate the model's performance on the test dataset and print the loss, accuracy, and classification report to assess the model's prediction quality.\n",
        "\n",
        "Model Class (**DistilBERTWithMetadata):**\n",
        "\n",
        "The unchanged model class is used here, which combines DistilBERT for text processing with metadata features for classification.\n",
        "\n",
        "\n",
        "**Loss Function:**\n",
        "\n",
        "Cross-entropy loss is used for multi-class classification.\n",
        "\n",
        "\n",
        "\n",
        "**Evaluation Mode:** The model is set to evaluation mode (**model.eval()**) to ensure no gradients are computed.\n",
        "\n",
        "Loss and Accuracy Calculation:\n",
        "\n",
        "The model is evaluated on the test dataset **(test_loader)**, and the results (loss, accuracy, classification report) are printed.\n",
        "\n"
      ],
      "metadata": {
        "id": "aWi9RqKpPMu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model class (unchanged)\n",
        "class DistilBERTWithMetadata(nn.Module):\n",
        "    def __init__(self, metadata_dim, dropout=0.3):\n",
        "        super(DistilBERTWithMetadata, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.metadata_fc = nn.Linear(metadata_dim, 64)\n",
        "        self.classifier = nn.Linear(768 + 64, 2)\n",
        "        self.text_weight = 0.8\n",
        "        self.metadata_weight = 0.2\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, metadata):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.last_hidden_state[:, 0]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        metadata_output = torch.relu(self.metadata_fc(metadata))\n",
        "        metadata_output = self.dropout(metadata_output)\n",
        "\n",
        "        weighted_text = self.text_weight * pooled_output\n",
        "        weighted_metadata = self.metadata_weight * metadata_output\n",
        "        combined = torch.cat((weighted_text, weighted_metadata), dim=-1)\n",
        "\n",
        "        logits = self.classifier(combined)\n",
        "        return logits\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Initialize model and load saved weights\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DistilBERTWithMetadata(metadata_dim=len(metadata_cols)).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/distilbert_finetuned.pt'))\n",
        "model.eval()\n",
        "\n",
        "# Loss function\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "# Evaluation function with classification report\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            metadata = batch['metadata'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask, metadata)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds, target_names=['ham', 'spam'])\n",
        "    accuracy = (sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels))\n",
        "\n",
        "    return avg_loss, accuracy, report\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc, test_report = evaluate(model, test_loader, device)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "print('\\nDetailed Classification Report:')\n",
        "print(test_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcEN4t3lMsSG",
        "outputId": "a8e5645f-6957-4427-c793-140c79f35105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Test Loss: 0.0382, Test Accuracy: 0.9912\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99      2538\n",
            "        spam       0.98      0.98      0.98       741\n",
            "\n",
            "    accuracy                           0.99      3279\n",
            "   macro avg       0.99      0.99      0.99      3279\n",
            "weighted avg       0.99      0.99      0.99      3279\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference on Test Data**\n",
        "\n",
        "\n",
        "*   This block demonstrates how to load a pre-trained model and tokenizer from Google Drive, and make predictions on a new test batch consisting of email texts and metadata features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Using the trained model to classify a batch of email texts as spam or ham. This block also demonstrates how to process text and metadata input for prediction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VF8NWz8RPbR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load tokenizer\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Define the model class\n",
        "class DistilBERTWithMetadata(nn.Module):\n",
        "    def __init__(self, metadata_dim, dropout=0.3):\n",
        "        super(DistilBERTWithMetadata, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.metadata_fc = nn.Linear(metadata_dim, 64)  # metadata_dim should match training\n",
        "        self.classifier = nn.Linear(768 + 64, 2)\n",
        "        self.text_weight = 0.8\n",
        "        self.metadata_weight = 0.2\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, metadata):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.last_hidden_state[:, 0]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        metadata_output = torch.relu(self.metadata_fc(metadata))\n",
        "        metadata_output = self.dropout(metadata_output)\n",
        "\n",
        "        weighted_text = self.text_weight * pooled_output\n",
        "        weighted_metadata = self.metadata_weight * metadata_output\n",
        "        combined = torch.cat((weighted_text, weighted_metadata), dim=-1)\n",
        "\n",
        "        logits = self.classifier(combined)\n",
        "        return logits\n",
        "\n",
        "# Initialize model with correct metadata_dim (must match training)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "metadata_cols = ['sender_score', 'time']  # Use 2 features to match training (was 2 in checkpoint)\n",
        "model = DistilBERTWithMetadata(metadata_dim=len(metadata_cols)).to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/distilbert_finetuned.pt'))\n",
        "model.eval()\n",
        "\n",
        "# Example test batch with 2 metadata features\n",
        "test_texts = [\n",
        "    \"This is a legitimate email about your order confirmation.\",\n",
        "    \"Win a free prize now! Click here immediately!!!\"\n",
        "]\n",
        "labels = torch.tensor([0, 1])  # 0 = ham, 1 = spam\n",
        "metadata = torch.tensor([\n",
        "    [1.0, 0.5],  # Metadata for ham (e.g., sender_score, time)\n",
        "    [0.1, 0.9]   # Metadata for spam (e.g., suspicious sender, recent time)\n",
        "]).float()  # Ensure float type for linear layer\n",
        "\n",
        "# Tokenize text\n",
        "inputs = tokenizer(test_texts, return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
        "input_ids = inputs['input_ids'].to(device)\n",
        "attention_mask = inputs['attention_mask'].to(device)\n",
        "metadata = metadata.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask, metadata)\n",
        "    _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "# Print results\n",
        "for i in range(len(test_texts)):\n",
        "    print(f\"Text: {test_texts[i]}\")\n",
        "    print(f\"True Label: {labels[i].item()} (0 = ham, 1 = spam)\")\n",
        "    print(f\"Predicted Label: {predicted[i].item()} (0 = ham, 1 = spam)\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSlnporrS7YH",
        "outputId": "ab3e4730-c6a1-46b2-92b8-003b4b4ae792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Text: This is a legitimate email about your order confirmation.\n",
            "True Label: 0 (0 = ham, 1 = spam)\n",
            "Predicted Label: 0 (0 = ham, 1 = spam)\n",
            "---\n",
            "Text: Win a free prize now! Click here immediately!!!\n",
            "True Label: 1 (0 = ham, 1 = spam)\n",
            "Predicted Label: 1 (0 = ham, 1 = spam)\n",
            "---\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}